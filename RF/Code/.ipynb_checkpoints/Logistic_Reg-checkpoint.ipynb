{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "\n",
    "Logistic Regression is a Supervised Machine Learning **classification** algorithm. We used it to predict the probability that an event will occur. The response variable (what we try to predict), is a binary variable. In other words it has two classes (i.e. pass/ fail). Often represented as 0 and 1 respectively. The Logistic Regression Model predicts $P(y=1|X)$, which reads as the probability that $y$ will happen given $X$, where $X$ is the set of features (see lecture notes). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement \n",
    "\n",
    "Heart disease is a global health threat. World Health Organisation (WHO) estimates that 17 million deaths occur worldwide due to Heart disease https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1. Early detection of heart-disease signs can greatly help reducing the death risk. In this project, we will use Logistic Regression to predict the overall risk of having a heart condition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Source \n",
    "\n",
    "The dataset used in this tutorial is obtained from the UCI data repository. Details about the data and relevant papers can be found here https://archive.ics.uci.edu/ml/datasets/Heart+Disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "Load the necessary libraries numpy arrays, pandas for data frames, seaborn for visualisation and matplitlib for visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the file \n",
    "* View few records just to see how the data look like\n",
    "* Check the dimension of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/heart.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size \n",
    "print(f'The dataset contains {df.shape[0]} rows, and {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always good to check the names of the columns \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Types**: The data contains mostly numerical values, which makes it easy to deal with. However, three features **cp**, **thal** and **slope**, are categorcial variables (they take the value 1, 2 or 3, etc...and we will later learn how to handle these). However, to double check the data types we can issue the following Python statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and check the data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Values**: An important piece of information about the dataset, is the unique values that are present in each feature. We know that the target value (class label), has either 0 or one value, but what about the rest of features? Here is how can obtain this information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 41\n",
      "sex 2\n",
      "trestbps 49\n",
      "chol 152\n",
      "fbs 2\n",
      "thalach 91\n",
      "exang 2\n",
      "oldpeak 40\n",
      "ca 5\n",
      "target 2\n",
      "cp_0 2\n",
      "cp_1 2\n",
      "cp_2 2\n",
      "cp_3 2\n",
      "thal_0 2\n",
      "thal_1 2\n",
      "thal_2 2\n",
      "thal_3 2\n",
      "slope_0 2\n",
      "slope_1 2\n",
      "slope_2 2\n"
     ]
    }
   ],
   "source": [
    "# for every column \n",
    "for i in df.columns:\n",
    "    # print how many features it has \n",
    "    print(i,len(df[i].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex has 2 unique values, These are: [1 0]\n",
      "fbs has 2 unique values, These are: [1 0]\n",
      "exang has 2 unique values, These are: [0 1]\n",
      "ca has 5 unique values, These are: [0 2 1 3 4]\n",
      "target has 2 unique values, These are: [1 0]\n",
      "cp_0 has 2 unique values, These are: [0 1]\n",
      "cp_1 has 2 unique values, These are: [0 1]\n",
      "cp_2 has 2 unique values, These are: [0 1]\n",
      "cp_3 has 2 unique values, These are: [1 0]\n",
      "thal_0 has 2 unique values, These are: [0 1]\n",
      "thal_1 has 2 unique values, These are: [1 0]\n",
      "thal_2 has 2 unique values, These are: [0 1]\n",
      "thal_3 has 2 unique values, These are: [0 1]\n",
      "slope_0 has 2 unique values, These are: [1 0]\n",
      "slope_1 has 2 unique values, These are: [0 1]\n",
      "slope_2 has 2 unique values, These are: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# print these unique values for columns that have less than 10 unique values \n",
    "# for every column \n",
    "for i in df.columns:\n",
    "    # print how many features it has \n",
    "    if len(df[i].unique())<10:\n",
    "        print(f'{i} has {len(df[i].unique())} unique values, These are: {df[i].unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe**: Another useful function in Pandas is the **describe** function. It provides you with quick statistical information about the data set. The information below should be self explanatory (i.e. count, mean, std stands for standard deviation, min, max and the 25 quartiles, etc...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The table above shows that **25%** of patients in this dataset are below the age of **47**, **50%** of patients in this dataset are below the age of **55**, and so on. If you want to better understand these numbers, consider the simple code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of people under the age of 42 is  25.0\n"
     ]
    }
   ],
   "source": [
    "# number of rows of people in the dataset with age<42\n",
    "n = df[df.age<=47].shape[0]\n",
    "\n",
    "# print the percentage of those \n",
    "print ('Percentage of people under the age of 42 is ', round(n/df.shape[0],2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of people under the age of 49 is  0.5\n"
     ]
    }
   ],
   "source": [
    "# similarily for the 50% percentile \n",
    "# number of rows of people in the dataset with age<42\n",
    "n = df[df.age<=55].shape[0]\n",
    "\n",
    "# print the percentage of those \n",
    "print ('Percentage of people under the age of 49 is ', round(n/df.shape[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same numbers obtained using the **describe()** function above regarding the quantiles, can also be accessed using hte pandas function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.quantile([.1, .25, .5, .75], axis = 0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Distribution** \n",
    "\n",
    "One of the important things you always need to check in your dataset, is the class distribution. This is an important research problem, and if the class-distribution of the dataset is largely imbalanced, then it could affect the performance of the machine learning algorithm. This can be done easily as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to visualise this distribution, among other aspects of the dataset. We can do this using the **seaborn** python library. But, first we will create a copy of the dataset, change some values to give it more meaningful names (i.e. 0 will be changed to male, and so on):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# first we make a copy of the dataset to decode the variables (for visualisation purposes)\n",
    "dfC = df.copy()\n",
    "# simple function to change sex values\n",
    "def changeS(sex):\n",
    "    if sex == 0:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Male'\n",
    "\n",
    "# apply the above function to the whole column in the dataset\n",
    "dfC['sex'] = dfC['sex'].apply(changeS)\n",
    "# function to change taget\n",
    "def changeT(label):\n",
    "    if label == 0:\n",
    "        return 'Heart Disease'\n",
    "    else:\n",
    "        return 'No Heart Disease'\n",
    "dfC['target'] = dfC['target'].apply(changeT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above step may not be necessary, it does help producing a meaningful annotation of the plots. The code below produces a figure that shows the distribution of patients with /with heart condition according to the gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(9,7)})\n",
    "\n",
    "sns.countplot(data= dfC, x='sex',hue='target')\n",
    "plt.title('Gender v/s target\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Plot is another useful visualisation that helps you gain more understanding about the dataset. Below is the code to generate a diagram that shows the 4 quartiles in of patients age grouped by having a heart condition or not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13553e7d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=dfC,x='target',y='age')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# you could always add condition to your code to explore specific category of patients (i.e. females)\n",
    "sns.boxplot(data=dfC[dfC.sex=='Female'],x='target',y='age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to check the percentages of patients with and without heart conditions. There are so many ways to do thing in python, here is one way. It should be noted that this data is not imbalanced, as can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_with_disease = len(df[df.target==1])\n",
    "patients_no_disease = len(df[df.target==0])\n",
    "print(f'Percentage of Patients with Heart Conditions is {round(patients_with_disease/df.shape[0]*100,2)}' )\n",
    "print(f'Percentage of Patients with no Heart Conditions is {round(patients_no_disease/df.shape[0]*100,2)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a lot of useful features that you can use to explore that data. One of these is the **groupby( )** function. Below, we will use it to look at the mean value of all measurements grouped by the target value (class label).  For example, it can be noted from below, that the average age of patients with no disease is 56, while for those of disease is 52. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also group by more than one variable as below\n",
    "df.groupby(['sex','target']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also condition on some values, then groupby. Below, we want to show the average age of patients with no disease who are women? Notice that 1 = male, and 0 = women ) / https://archive.ics.uci.edu/ml/datasets/heart+disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or condition and then group by. \n",
    "df[df.sex==0].groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average age of men with disease? \n",
    "df[(df.sex==1) &(df.target==1)].groupby('target').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables \n",
    "\n",
    "According to the dataset description four features **cp**, **thal** and **slope**, and **restecg**  are categorical variables (they have discrete values). We can check this as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show few values of the above four columns \n",
    "df[['cp','thal','slope','restecg']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets check the unique values of these categorical variables \n",
    "cat_variables = ['cp','thal','slope','restecg']\n",
    "for i in cat_variables:\n",
    "    # print how many features it has \n",
    "    print(f'{i} has {len(df[i].unique())} unique values, These are: {df[i].unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above values of the categorical variables 'cp','thal','slope','restecg', represent categories, and not numerical values, and therefore, we need to turn these categorical values into dummy variables as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.get_dummies(df['cp'], prefix = \"cp\")\n",
    "b = pd.get_dummies(df['thal'], prefix = \"thal\")\n",
    "c = pd.get_dummies(df['slope'], prefix = \"slope\")\n",
    "d = pd.get_dummies(df['slope'], prefix = \"restecg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy Variables**: The statements above, creates four different dataframes, each represent one categorical variable. Check below how the variable **cp** is changed into four different columns (**'cp_0', 'cp_1', 'cp_2', 'cp_3**), and the same for the other dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, lets drop the original categorical variables from the dataframe **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(cat_variables, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will add the generated dummy variables to the dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine all data frames \n",
    "df_tmp = [df, a, b, c] \n",
    "df = pd.concat(df_tmp, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values \n",
    "\n",
    "Most datasets will have some form of missing values. So, it is important to check for any missing values within the data. This can be easily done in Python as follows; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "ca          0\n",
       "target      0\n",
       "cp_0        0\n",
       "cp_1        0\n",
       "cp_2        0\n",
       "cp_3        0\n",
       "thal_0      0\n",
       "thal_1      0\n",
       "thal_2      0\n",
       "thal_3      0\n",
       "slope_0     0\n",
       "slope_1     0\n",
       "slope_2     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that according to the above results, we have no missing value in the dataset. However, this is not often the case with most datasets. There are various ways to handle missing values. Simplest way is to compute the mean value of a certain feature (column in the dataset), then assign that mean value to all missing values in the same column. Suppose that the variable **fbs** has some missing values, Below is the code to fill this variable missing values with the mean of that variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for fbs \n",
    "fbs_mean = round(df['fbs'].mean())\n",
    "# assign the mean to the missing values \n",
    "df['fbs'].fillna(fbs_mean, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your Logistic Regression Model \n",
    "\n",
    "The logistic Regression, is simple and easy to implement. It is one of the very common used algorithms in the industry. To better understand it, recall the linear regression that we discussed last week which is defined as $\\hat{Y}=\\theta_0+\\theta_1 X_1+\\theta_2 X_2+\\cdots+\\theta_p X_k+\\epsilon$, and used for regression problems (i.e. predicting continuous variable). Similarly, Logistic Regression  is used for **classification** and is expressed as $\\hat{Y}=\\textbf{g}(\\theta_0+\\theta_1 X_1+\\theta_2 X_2+\\cdots+\\theta_p X_k+\\epsilon)$. Notice, the **g** function, which is a non-linear function. Although, the model can be implemented from scratch, we want here to use **sklearn** Python library to do so: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Features and Target Variables \n",
    "\n",
    "First, we need to prepare the data by splitting the dataframe (df) into a set of features **X** and a Target variable **y**. This is can be easily done as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the input features \n",
    "X = df.drop('target', axis = 1)\n",
    "# store the label column into y variable \n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the code below to check the type of x, y\n",
    "\n",
    "#print(type(X))\n",
    "#print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data \n",
    "\n",
    "This is very important step,  you need to hold part of your data (test set) to use after finishing the training process, to evaluate the performance of your model. This is one of the important way to ensure that your model is not **overfitting**. Below, we use the **train_test_split** to do this split (80% of the data for training, and 20% of the data for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising Data\n",
    "\n",
    "Min-Max Normalization method is used to Normalize the data. This method scales the data range to [0,1]. This is a good practice to put the data into a common scale and it helps supress the noise in the data. Normalising your data is simple, and can be expressed as follows:  $\\Large x_{scaled}=\\frac{x-x_{min}}{x_{max}-x_{min}}$, and can be implemented in Python as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "normalized_X_train = preprocessing.normalize(X_train)\n",
    "normalized_X_test = preprocessing.normalize(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit the Model \n",
    "\n",
    "We use the **LogisticRegression** from the **sklearn** library to create the Logistic Regression Model. Notice the **fit( )** method we use to fit our data into the model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# notice that you can change the parameters of the model\n",
    "lg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "lg_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Manual Error Inspection \n",
    "\n",
    "Lets first use our model to perform predictions on the dataset. Notice that we only pass the input features, and we will let the model predict the class. We can then compare the predicted values against the ground truth and further inspect the results. This is not a must-do step, but helps greatly understand the model's performance on specific instances: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values \n",
    "y_pred = lg_model.predict(X_test)\n",
    "# actual values \n",
    "actual = y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is good to do some manual inspections for the results. One way to do so is to save the predicted values, the actual values, and a column indicating if both values are equal in one data frame. Below is the code on how to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary and a data frame and check correct predictions \n",
    "dic = {'Actual':actual,\n",
    "       'Prediction':y_pred,\n",
    "       'correct_prediction':0\n",
    "       }\n",
    "\n",
    "result  = pd.DataFrame(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the correct_prediction values to 1, if correct prediction\n",
    "def myfunc(x,y):\n",
    "    if x == y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "result['correct_prediction'] = result.apply(lambda x: myfunc(x.Actual, x.Prediction), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice now, you can view only the patients data that records that were incorrectly classified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.correct_prediction==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can now manually compute the accuracy of your model. We will later compare this with the results we obtain from the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model Accuracy on Test Data is {len(result[result.correct_prediction==1])/len(result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can save the dataframe of the results for further inspection \n",
    "result.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "As discussed in the lecture, there are several metrics, that we can use to evaluate the modelâ€™s performance. You often don't need to do this manually as we did above. Below, we will again make use of the sklearn library to get the overall accuracy of the model: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Accuracy on Testing Set is 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'The Model Accuracy on Testing Set is {round(accuracy_score(y_test,y_pred),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also get other metrics such as precision, recall and f1-score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report of the model above shows that 88% prediction of absence of heart disease was predicted correct and 86% of presence of heart disease was predicted correct. One more useful metric to consider as we discussed in the lecture is the confusion matrix: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\")\n",
    "ax.set_ylim([0,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix True Positive value is 31 and true Negative was 22. And the False Positive came out to be 3 and False Negative is 5.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your Model\n",
    "\n",
    "In the following few lines of code, our logistic regression model that created in the above sections will be saved into a file. Recall that our model above is named `lg_model`. Saving the model is important step for re-using the model, without the need to retrain it. Our model will be loaded as a new object called `pickled_model`. Once loaded, we can use it to calculate accuracy scores and do predictions as we did above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 86.89 %\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_lr_model.pkl\"\n",
    "# save your model that was created above (lg_model) \n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(lg_model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Lets test the mode loaded from a file and check results \n",
    "score = pickle_model.score(X_test, y_test)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "y_hat = pickle_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that now if you check your current working directory, then you should find a file called `pickle_lr_model.pkl`. It must be noted also that you can use this model now in a `streamlit` application as we did in the previous week. If you have time, you should try this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
